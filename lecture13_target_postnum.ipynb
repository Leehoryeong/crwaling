{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 필요한 모듈 import\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크룰링할 키워드: 양산여행\n",
      "크롤링할 건수는 몇건입니까?100\n",
      "검색 결과를 저장할 TXT파일 경로와 이름 지정: C:\\Users\\01048\\Desktop\\crawling\\result\\test2.txt\n",
      "검색 결과를 저장할 CSV파일 경로와 이름 지정C:\\Users\\01048\\Desktop\\crawling\\result\\test2.csv\n",
      "검색 결과를 저장할 xlsx파일 경로와 이름 지정C:\\Users\\01048\\Desktop\\crawling\\result\\test2.xlsx\n"
     ]
    }
   ],
   "source": [
    "# step2 필요한 정보를 입력받자\n",
    "\n",
    "query_txt = input(\"크룰링할 키워드: \")\n",
    "cnt = int(input(\"크롤링할 건수는 몇건입니까?\"))\n",
    "page_cnt = math.ceil(cnt/10)\n",
    "\n",
    "f_name = input(\"검색 결과를 저장할 TXT파일 경로와 이름 지정: \")\n",
    "fc_name = input(\"검색 결과를 저장할 CSV파일 경로와 이름 지정\")\n",
    "fx_name = input(\"검색 결과를 저장할 xlsx파일 경로와 이름 지정\")\n",
    "#  C:\\Users\\01048\\Desktop\\crawling\\result\\test1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 크롬 웹드라이버로 웹 브라우저 실행\n",
    "path = 'chromedriver_240.exe'\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "driver.get('https://korean.visitkorea.or.kr/main/main.html') #홈페이지 열어\n",
    "time.sleep(2) # 위 페이지가 모두 열릴 때까지 2초 대기\n",
    "\n",
    "# step2 검색창을 찾아서 검색어 입력\n",
    "driver.find_element_by_id('btnSearch').click()\n",
    "\n",
    "element = driver.find_element_by_id('inp_search')\n",
    "\n",
    "element.send_keys(query_txt)\n",
    "\n",
    "# step3 검색 버튼 실행\n",
    "#dirver.find_element_by_link_text('검색').click()\n",
    "driver.find_element_by_id('//*[@id=\"gnbMain\"]/div/div/div[1]/div[1]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "요청하신 검색건수는 120건\n",
      "해당 키워드의 실제 검색된 글의 수는 101건\n",
      "실제검색 건수를 101건으로 재설정\n",
      "총 12개의 페이지의 게시글 수집\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cnt2=0\n",
    "time.sleep(2)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "search_cnt_1 = soup.find('div','total_check')\n",
    "search_cnt_2 = search_cnt_1.find('strong').find('span').get_text() #안의 태그로 파고들기 \n",
    "\n",
    "if cnt > int(search_cnt_2):\n",
    "    cnt2 = int(search_cnt_2)\n",
    "    \n",
    "real_page_cnt = math.ceil(cnt/10)\n",
    "\n",
    "print('\\n')\n",
    "print('-' * 80)\n",
    "print('요청하신 검색건수는 %s건' %cnt)\n",
    "print('해당 키워드의 실제 검색된 글의 수는 %s건' %cnt2)\n",
    "print('실제검색 건수를 %s건으로 재설정' %cnt2)\n",
    "print('총 %s개의 페이지의 게시글 수집' %real_page_cnt)\n",
    "print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for x in (페이지 번호 반복 범위):\n",
    "    for y in (게시글 번호 반복 범위):\n",
    "        제목 찾아와\n",
    "        해시태그 찾아와\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = 1 #게시글 번호 카운트 용\n",
    "no2 = [] #게시글 번호 저장용 리스트\n",
    "contents2 = [] #게시글 내용 저장용 리스트\n",
    "tags2 = [] #해시 태그 정보 저장용 리스트\n",
    "\n",
    "for x in range(1, real_page_cnt+1):\n",
    "    print('%s페이지 정보 수집 시작합니다.'%x)\n",
    "    print('\\n')\n",
    "    print('-'*80)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    content_list = soup.find('ul','list_thumType flnon').find_all('div','area_txt')\n",
    "    \n",
    "    for i in content_list:\n",
    "        no2.append(no)\n",
    "        print('번호:',no)\n",
    "        \n",
    "        contents = i.find('div','tit').get_text()\n",
    "        contents2.append(contents)\n",
    "        print('내용:',contents.strip())\n",
    "        try:\n",
    "            tag = i.find('p','tag').get_text()\n",
    "        except AttributeError:\n",
    "            tag = '태그정보가 없습니다.'\n",
    "            tags2.append(tag)\n",
    "            print('태그',tag.strip())\n",
    "            print('\\n')\n",
    "        else:\n",
    "            tags2.append(tag)\n",
    "            print('태그',tag.strip())\n",
    "            print('\\n')\n",
    "        if no==cnt2:\n",
    "            break\n",
    "        \n",
    "        no += 1\n",
    "    time.sleep(2)\n",
    "    x += 1\n",
    "    \n",
    "    if x == real_page_cnt+1:\n",
    "        break\n",
    "    if x == 6 or x == 11:\n",
    "        driver.find_element_by_xpath('//*[@id=\"6\"]').click()\n",
    "    else:\n",
    "        driver.find_element_by_link_text('%s'%x).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 블로그 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크룰링할 키워드: 여름여행\n",
      "포함할 키워드: 국내\n",
      "제외할 키워드: 해외\n",
      "크롤링할 건수: 123\n"
     ]
    }
   ],
   "source": [
    "query_txt = input(\"크룰링할 키워드: \")\n",
    "po_txt = input(\"포함할 키워드: \")\n",
    "je_txt = input(\"제외할 키워드: \")\n",
    "cnt = input(\"크롤링할 건수: \")\n",
    "cnt = int(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'chromedriver_240.exe'\n",
    "driver = webdriver.Chrome(path)\n",
    "url = 'https://www.naver.com/'\n",
    "driver.get(url)\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = driver.find_element_by_class_name('input_text')\n",
    "element.send_keys(query_txt)\n",
    "driver.find_element_by_id('search_btn').click()\n",
    "time.sleep(1)\n",
    "driver.find_element_by_link_text('블로그').click()\n",
    "time.sleep(1)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "blog_list = soup.find('ul','type01').find_all('dl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_num = math.ceil(cnt/10)\n",
    "title_list = []\n",
    "summary_list = []\n",
    "num = []\n",
    "no = 0\n",
    "cnt2= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,page_num+1):\n",
    "    blog_list = soup.find('ul','type01').find_all('dl')\n",
    "    for post in blog_list:\n",
    "        title = post.find('dt').get_text()\n",
    "        summary = post.find('dd','sh_blog_passage').get_text()\n",
    "        if (je_txt not in post):\n",
    "            title_list.append(title)\n",
    "            summary_list.append(summary)\n",
    "            no += 1\n",
    "            num.append(no)\n",
    "            if no == cnt:\n",
    "                break\n",
    "    cnt2 += 1\n",
    "    driver.find_element_by_link_text('%s'%cnt2).click()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
